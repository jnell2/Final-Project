Day 1:
10/31/2016
Overwhelmed with data, so I did research to try to find similar projects that have already been done.
No Githubs, but a few papers. Used very basic measure and ran models achieving a maximum accuracy around 60%.
Struggled through web scraping, found out I had to use selenium.
Finally got my first data table loaded into a pandas dataframe.

Day 2:
11/01/2016
Adjusted web-scraping code to get other data tables I needed.
All are loaded into pandas dataframes.
Trying to scrape nhl standings, but running into 2 issues:
1) website is same for all tabs, need to figure out how to have scraping code actually look at the "league" tab
2) the first column of the table has 3 elements, need to figure out how to get all 3 elements.

Day 3:
11/02/2016
Up until 1:00, was struggling with the nhl standings webpage trying to scrape the nhl standings.
Tried several different approaches, kept funning into issues.
Started looking at other sources where I could get the nhl standings.
Figured out that if I sort a table that I have already scraped in a certain way, I can get the standings.
(Table: Season Team Summary, Sort: points, wins, games played)

From here, I will compile the final dataset that I will work with. I have 7 tables that I scraped.
3 are game-by-game stats and 4 are season stats.
I want my final data table to be cumulative stats for each team for the past 10 games.
(or seasonal stats, like standings, missed shots [corsi, fenwick], etc.)
There will be 30 rows (30 teams).
For now, I think I will do this using pandas dataframes, just to save time since I wasted so much trying to get the standings.
I will end up having 2 final pandas dataframes:
1) the total table with all games, game-by-game, from the specified time period (only GbG data)
2) the table with each team having a single row, add in season data, calculate fields
First steps will be to merge my GbG pandas dataframes to keep all of the columns that I want.
This will be df 1.
Next steps will be to combine rows in pandas dataframe for past 10 games.
Then I will add the SbS dataframe information that I want to keep.
Then I will add calculated columns.
This will be df 2.

Day 4:
11/03/2016
Just realized last night that I need a different final data table.
In order to predict wins, I need to keep information in a game-by-game format.
So I will need 3 separate dataframes to get here.
1) the total table with all games, game-by-game, from the specified time period (only GbG data)
2) the table with each team having a single row (past 10 games) , add in season data, calculate fields
3) the table with home/away teams and an indicator for whether or not the home team won.
This will come from the GbG data. Then for each game, I will supplement the information from table 2.

Issues: had to fix non-ascii characters. Only getting first page of data for GbG data.
Need to do: get the rest of the pages, clean pandas dfs, save as csvs, use postgres to get final data tables.

Day 5:
11/04/2016
To do today: get the rest of the pages for the data tables, clean pandas dfs,
save as csvs, use postgres to get final data tables.
Maybe do some work this weekend to get further ahead--I've run into a lot of issues.

Finally got WebScraping.py working correctly!

Suggestion from Adam: maybe predict the goal spread instead of W/L
Simple web app with home/away team, logo, and button to predict
Good idea to look at length of time.

Need to do this weekend:
1) clean up pandas dataframes
2) save dataframes as csv
3) combine tables to get the tables that I want. want to have different time periods.
4) if time, start running some models.

Day 6:
11/06/2016
all dataframes are being collected nicely
automated some of the process. no longer need to change page numbers or any dates.
next steps are to clean up the dataframes and save these as csvs.
if i get to this point, i'm good to go for monday. 
